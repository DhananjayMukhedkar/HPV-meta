{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge BAM\n",
    "\n",
    "This job to merge BAM files per sample using samtools. Unique files are found by combining:\n",
    "\n",
    "* Lanes\n",
    "* Paired/Unpaired\n",
    "* All parts incase the input FASTQ was split into smaller parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import pysam\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "import stat\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_full=utils.load_arguments(sys.argv)\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full[utils.KEY_MERGE]\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    INPUT_ROOT=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    INPUT_ROOT=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    OUTPUT_MERGE=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    OUTPUT_MERGE=os.path.join(WORK_PATH,args['OUTPUT_MERGE'])\n",
    "\n",
    "THREADS=str(args['THREADS'])\n",
    "tool_path=args['SAMTOOLS_PATH']\n",
    "TOOL='./samtools/bin/samtools'\n",
    "SPACE=utils.SPACE\n",
    "\n",
    "def chmod_exec(tool):\n",
    "    st = os.stat(tool)\n",
    "    os.chmod(tool, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "def install_samtools(tool_path):\n",
    "    hdfs.copy_to_local(tool_path)\n",
    "    lib1='./samtools/bin/samtools'\n",
    "    chmod_exec(lib1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Function to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_files(file):\n",
    "    \"\"\"\n",
    "    runs 'samtools merge' as subprocess to merge all files for a given sample name.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"INFO: Run merge files\", file)    \n",
    "    first_name=file\n",
    "    merged_file=first_name+'.bam'\n",
    "\n",
    "    if not os.path.exists('samtools'):\n",
    "        install_samtools(tool_path)\n",
    "\n",
    "    if utils.skip_file(file,merged_file,OUTPUT_MERGE):\n",
    "        return [-1]\n",
    "\n",
    "    group_files=list(filter(lambda x: file in x ,inputFiles)) # all files for sample name\n",
    "    if len(group_files)==1: # if only single file copy to hdfs output folder\n",
    "        hdfs.cp(os.path.join(INPUT_ROOT,group_files[0]), os.path.join(OUTPUT_MERGE,group_files[0]),overwrite=True )\n",
    "        return merged_file\n",
    "    # copy all files to local\n",
    "    [hdfs.copy_to_local(os.path.join(INPUT_ROOT,x), overwrite=True) for x in group_files]\n",
    "    # get string with all file names\n",
    "    args=' '.join(group_files)\n",
    "    \n",
    "    params={'merge':merged_file, args: '','-@': THREADS}\n",
    "    cmd=utils.build_command(TOOL,params)\n",
    "    try:\n",
    "        status=subprocess.run(cmd.split(),stdout=subprocess.PIPE,check=True)\n",
    "        if status.returncode==0 and os.path.exists(merged_file):\n",
    "            hdfs.copy_to_hdfs(merged_file,OUTPUT_MERGE,overwrite=True)\n",
    "            os.remove(merged_file)\n",
    "    except subprocess.SubprocessError:\n",
    "        traceback.print_exc()\n",
    "        # delete corrupted input files\n",
    "        print('INFO: Input files could be corrupted, deleting input files: ', group_files)\n",
    "        [hdfs.delete(os.path.join(INPUT_ROOT,x)) for x in group_files]\n",
    "    finally:\n",
    "        for f in group_files:\n",
    "            os.remove(f)\n",
    "\n",
    "    return merged_file\n",
    "  \n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input file hdfs paths\n",
    "inputFiles=utils.load_file_names(INPUT_ROOT)\n",
    "# take only file names\n",
    "inputFiles=[os.path.basename(f) for f in inputFiles]\n",
    "# get unique sample name\n",
    "uniques=utils.find_unique_names(inputFiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge in parallel\n",
    "mergedList=sc.parallelize(uniques,sc.getConf().get(\"spark.executor.instances\")).map(merge_files).collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-de71e280",
   "language": "python",
   "display_name": "PyCharm (KI_local)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}