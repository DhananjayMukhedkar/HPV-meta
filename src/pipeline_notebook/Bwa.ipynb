{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "import utils\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Argument Parser for BWA\")\n",
    "parser.add_argument(\"-i\", \"--input\", help=\"Input root HDFS path\")\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"Output root HDFS path \")\n",
    "\n",
    "options = parser.parse_args(sys.argv[1:])\n",
    "if options.input:\n",
    "    INPUT_ROOT=options.input\n",
    "else :\n",
    "    raise ValueError(\"Input path missing\")\n",
    "if options.output:\n",
    "    OUTPUT_ROOT=options.output\n",
    "else:\n",
    "    raise ValueError(\"Output path missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR='Logs/Bwa'\n",
    "\n",
    "REFERENCE_FASTA_PATH = \"References/HPVproteinsincludingnonoficial_201119.faa\"\n",
    "OUTPUT_PREFIX='alg_'\n",
    "SAI_FORMAT='.sai'\n",
    "SAM_FORMAT='.sam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_bwa(file,ref_path):\n",
    "    # download diamond DB file\n",
    "    ref=os.path.split(ref_path)[1]\n",
    "    if not os.path.exists(ref):\n",
    "        hdfs.copy_to_local(ref_path)\n",
    "\n",
    "    # index\n",
    "    cmd='bwa index '+ref\n",
    "    status=subprocess.run(cmd.split(),stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "\n",
    "    # download input file\n",
    "    hdfs.copy_to_local(file, overwrite=True)\n",
    "    input_file=os.path.basename(file)\n",
    "    filename=os.path.splitext(os.path.splitext(input_file)[0])[0] # split file name without extension\n",
    "\n",
    "    outfile=filename+SAI_FORMAT\n",
    "\n",
    "    parameters = { 'aln': ref, '':input_file}\n",
    "    cmd=utils.build_command(\"bwa\",parameters)\n",
    "    print('INFO: Running bwa with command:', cmd)\n",
    "\n",
    "    log_file=os.path.splitext(outfile)[0]+'.txt'\n",
    "    try:\n",
    "        with open(log_file, \"w\") as f:\n",
    "            with open(outfile,\"w\") as out:\n",
    "                execStatus=subprocess.run(cmd.split(),stdout=out,stderr=f,check=True)\n",
    "                if execStatus.returncode==0:\n",
    "                    cmd_sam='bwa samse'+' '+ref+' '+outfile+' '+input_file\n",
    "                    outputSam=OUTPUT_PREFIX+filename+SAM_FORMAT\n",
    "                    with open(outputSam,'w') as f_outSam:\n",
    "                        status_cmd2=subprocess.run(cmd_sam.split(),stdout=f_outSam,stderr=f,check=True)\n",
    "\n",
    "\n",
    "        hdfs.copy_to_hdfs(log_file, LOG_DIR, overwrite=True)\n",
    "\n",
    "        if status_cmd2.returncode==0 and os.path.exists(outputSam):\n",
    "            hdfs.copy_to_hdfs(outputSam,OUTPUT_ROOT,overwrite=True)\n",
    "            os.remove(outputSam)\n",
    "            os.remove(outfile)\n",
    "\n",
    "        return outputSam\n",
    "    except subprocess.SubprocessError:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    finally:\n",
    "        parameters.clear()\n",
    "        os.remove(input_file)\n",
    "        if os.path.exists(log_file):\n",
    "            os.remove(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load input data\n",
    "number_of_files=0\n",
    "\n",
    "inputFiles=[]\n",
    "if hdfs.isfile(INPUT_ROOT):\n",
    "    print(\" --- Input: is file ----reading from input file list\")\n",
    "    df=spark.read.csv(INPUT_ROOT,header=True)\n",
    "    number_of_files=df.count()\n",
    "    df_files=df.select('sorted_file').repartition(int(sc.getConf().get(\"spark.executor.instances\")))\n",
    "    rdd_names=df_files.rdd\n",
    "    # run\n",
    "    start=datetime.now()\n",
    "    mapped_rdd=rdd_names.map(lambda x:run_bwa(x[0],REFERENCE_FASTA_PATH))\n",
    "    mapped_rdd.collect()\n",
    "    print(\"time elapsed: \" +str(datetime.now() - start))\n",
    "else:\n",
    "    print(\" --- Input: is folder ----reading from input folder\")\n",
    "    inputFiles=utils.load_file_names(INPUT_ROOT)\n",
    "    number_of_files=len(inputFiles)\n",
    "    # run\n",
    "    rdd=sc.parallelize(inputFiles,sc.getConf().get(\"spark.executor.instances\"))\n",
    "    start=datetime.now()\n",
    "    final=rdd.map(lambda x: run_bwa(x,REFERENCE_FASTA_PATH) ).collect()\n",
    "    print(\"time elapsed: \" , datetime.now() - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "report_file='reportBwa_'+os.path.basename(INPUT_ROOT)+'.txt'\n",
    "with open(report_file,'w') as f:\n",
    "    f.write(\" Date:\" +str(datetime.now() ) )\n",
    "    f.write(\" \\n Number of input files: \" +str(number_of_files))\n",
    "    f.write(\" \\n Time elapsed: \" +str(datetime.now() - start))\n",
    "\n",
    "hdfs.copy_to_hdfs(report_file,'Experiments/benchmark/report',overwrite=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}