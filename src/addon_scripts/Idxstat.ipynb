{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering SAM \n",
    "\n",
    "This job to filter SAM files output from Nextgenmap. It runs `samtools` to convert to BAM and store only human unmapped content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>553</td><td>application_1615491195715_0581</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1615491195715_0581/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop18:8042/node/containerlogs/container_e41_1615491195715_0581_01_000001/TCGA_viruses__dhananja\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pysam\n",
    "import os\n",
    "from hops import hdfs\n",
    "import utils\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying hdfs:///Projects/TCGA_viruses/Jupyter/pipeline/settings_DJ.yml to local disk on path /srv/hops/hopsdata/tmp/nm-local-dir/usercache/rFfxgJCqYiFgJW_6YMkFOwNpaYJveIXICcW8hZxwcxQ/appcache/application_1615491195715_0581/container_e41_1615491195715_0581_01_000001/\n",
      "\n",
      "Finished copying"
     ]
    }
   ],
   "source": [
    "\n",
    "args=utils.load_arguments(sys.argv)\n",
    "#args=utils.load_arguments([0,'hdfs:///Projects/TCGA_viruses/Jupyter/pipeline/settings_DJ.yml'])\n",
    "if args is not None:\n",
    "    args=args['Idxstat']\n",
    "else :\n",
    "    sys.exit(utils.NO_CONFIG_ERR)\n",
    "    \n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputRoot=args['INPUT_BAM']\n",
    "inputBai=args['INPUT_BAI']\n",
    "OUTPUT_INDEX=args['OUTPUT_INDEX']\n",
    "OUTPUT_STAT=args['OUTPUT_STAT']\n",
    "\n",
    "threads=str(args['THREADS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8988"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def idx(file):\n",
    "    filename=os.path.basename(file)\n",
    "    idx_file=filename.split('.')[0]+'.txt'\n",
    "    bai_file=filename.split('.')[0]+'.bai'\n",
    "    \n",
    "    if not hdfs.exists(os.path.join(OUTPUT_STAT,idx_file)):    \n",
    "    \n",
    "        hdfs.copy_to_local(file, overwrite=True)\n",
    "        hdfs.copy_to_local(os.path.join(inputBai,bai_file))\n",
    "        ### running indexing if absent ###\n",
    "\n",
    "        #file=os.path.split(file)[1]\n",
    "\n",
    "        #print(\"INFO: Run indexing : \", file)    \n",
    "        #cmd='samtools index '+file + ' -@ '+ threads\n",
    "        #subprocess.run(cmd.split(),stdout=subprocess.PIPE)\n",
    "        \n",
    "\n",
    "        #pysam.index(file,'-@',threads, catch_stdout=False)\n",
    "\n",
    "        ### running indexing if absent ###\n",
    "        \n",
    "\n",
    "        if os.path.exists(bai_file):\n",
    "            #hdfs.copy_to_hdfs(bai_file,OUTPUT_INDEX,overwrite=True)\n",
    "                   \n",
    "\n",
    "            stdout=pysam.idxstats(filename,'-@',threads, catch_stdout=True)\n",
    "            with open(idx_file,'w') as f: \n",
    "                 f.write(stdout)\n",
    "\n",
    "            hdfs.copy_to_hdfs(idx_file,OUTPUT_STAT,overwrite=True)\n",
    "            os.remove(idx_file)\n",
    "            os.remove(bai_file)\n",
    "\n",
    "\n",
    "        os.remove(filename)\n",
    "    else :\n",
    "        print('skipping file', file)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputFiles=utils.load_file_names(inputRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775"
     ]
    }
   ],
   "source": [
    "len(inputFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TCGA_Data/chunk11'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "removed files due to samtools failure for corrupt files\n",
    "\"\"\"\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk4/18c4de42-b42e-44d6-90fc-f4130a2f76d6_gdc_realn_rehead.bam')\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk4/d9e0312b-86fd-4490-9626-4ba2e213d8ac_gdc_realn_rehead.bam')\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk5/d1ee552b-5219-47bd-be51-78d5467c4dd9_gdc_realn_rehead.bam')\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk7/e3a8f442-5157-4153-8ca1-9dfb11b8992c_gdc_realn_rehead.bam')\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk8/536b58ff-f385-4f1d-bb7a-47a52336e09d_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/2ef729a3-ef44-4e8f-ad03-539e86a30862_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/b303f535-1023-4e11-8cf9-fa9b9e81ef3a_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/108dc6d1-5612-4276-a682-0ba4d324cb00_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/a4739807-50c0-4da1-926a-4fc130d734fb_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/ba928d91-8f40-4e77-b67f-f7b5ba5b5520_gdc_realn_rehead.bam')\n",
    "# inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk9/bcdf0550-9e6f-4500-9473-706df7beeae4_gdc_realn_rehead.bam')\n",
    "\n",
    "\n",
    "##last chunk\n",
    "#inputFiles.remove('hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/TCGA_Data/chunk11/b303f535-1023-4e11-8cf9-fa9b9e81ef3a_gdc_realn_rehead.bam')\n",
    "\n",
    "\"\"\" end remove\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputFiles=['hdfs:///Projects/TCGA_viruses/TCGA_Data/chunk11/5b49dd79-a663-45bb-b56b-8b4b00b68191_gdc_realn_rehead.bam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputFiles=[inputFiles[0] ]\n",
    "# inputFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### convert to bam and filter unmapped sequences\n",
    "unMapped=sc.parallelize(inputFiles).map(idx).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}